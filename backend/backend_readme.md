# Optima

An ensemble NL-based optimization solver that compares solutions from OptiMUS and OptiMind, then uses an LLM judge to pick the best one.

Built for Treehacks 2026, with credit to original creators of OptiMUS and OptiMind.

## Setup

```bash
conda activate optima
pip install -r requirements.txt
```

Create a `.env` file in the project root (gitignored). Copy from `.env.example` and fill in your keys:

```
OPENAI_API_KEY=your-openai-key
ANTHROPIC_API_KEY=your-anthropic-key
OPTIMIND_SERVER_URL=http://<VM_IP>/v1

# Gurobi WLS (cloud license — works on any machine)
GRB_WLSACCESSID=your-access-id
GRB_WLSSECRET=your-secret
GRB_LICENSEID=your-license-id
```

### Gurobi license (for running generated solver code)

The pipeline generates and runs Gurobi (Python) code. You need a valid **Gurobi license** for that to work.

- **Get a license:** [Academic (free)](https://www.gurobi.com/academia/academic-program-and-licenses/) or [commercial](https://www.gurobi.com/licenses/). Use the [Gurobi User Portal](https://portal.gurobi.com/iam/licenses/list) to retrieve your license key.
- **Set it up** — two options:
  - **Option A — WLS (recommended for teams/deployment):** Add your Web License Service credentials to `.env` (see above). Works on any machine without a local license file.
  - **Option B — Local license file:** Run `grbgetkey` to download `gurobi.lic` to `~/gurobi.lic`. Only works on that machine.

If the license is not set up, running the pipeline or executing generated code will fail with a Gurobi license error.

---

## Quick Start

```bash
# 1. Place your files in data_upload/
#    - A .txt file with the problem description (required)
#    - A .csv file with parameter data          (optional)

# 2. Run the pipeline
python main.py
```

That's it. The script clears the workspace, processes your inputs, runs both solvers, judges the results, and writes the final verdict to `current_query/final_output/`.

### Alternative: explicit file paths

```bash
python main.py --desc path/to/problem.txt
python main.py --desc path/to/problem.txt --data path/to/params.csv
```

### Pipeline steps (what `main.py` does)

| Step | What happens |
|------|-------------|
| 1 | Archive + clear `current_query/` |
| 2 | Copy uploaded files into `current_query/raw_input/` (renamed to `raw_desc.txt` / `raw_params.csv`) |
| 3 | `raw_to_model` — LLM converts raw inputs to `model_input/desc.txt` + `params.json` |
| 4 | `optimus` + `optimind` — **run in parallel** (structured multi-step solver + single-pass LLM solver) |
| 5 | `judge` — compares both solutions, picks a winner |
| 6 | `consultant` — generates a professional Markdown report with baseline comparison |

If one solver fails, the other's result is still judged. If both fail, you get a clear error.

### LLM models

| Stage | Provider | Model |
|-------|----------|-------|
| raw_to_model | OpenAI | `gpt-4o` |
| OptiMUS (all pipeline steps) | Anthropic | `claude-sonnet-4-20250514` |
| OptiMind (solver) | Self-hosted (GCP) | `microsoft/OptiMind-SFT` |
| Judge (comparison) | OpenAI | `gpt-4o` |
| Consultant (report) | OpenAI | `gpt-4o` |

### CLI options

```
python main.py                                    # use data_upload/
python main.py --desc problem.txt                 # explicit desc file
python main.py --desc problem.txt --data data.csv # desc + CSV
python main.py --no-archive                       # skip archiving old results
python main.py --dir other_dir                    # different workspace
```

### Directory structure

```
data_upload/             Drop your input files here
  problem.txt              Problem description (.txt, required)
  data.csv                 Parameter data (.csv, optional)

current_query/           Working directory (managed automatically)
  raw_input/               Copied from data_upload/
    raw_desc.txt
    raw_params.csv
  model_input/             Generated by raw_to_model (+ frontend LLM in future)
    desc.txt                 Cleaned problem description
    params.json              Structured parameters
    baseline.txt             Client's current baseline strategy
  optimus_output/          Generated by optimus
  optimind_output/         Generated by optimind
  final_output/            Generated by judge + consultant
    verdict.json             Structured result (enriched by consultant)
    report.md                Professional Markdown report
```

---

## Individual Scripts

Each pipeline step can also be run independently for debugging or development.

### `query_manager.py`

Manages the `current_query/` workspace. Archives the current contents to `query_history/<timestamp>/` then deletes all files while preserving the subdirectory structure.

```
python query_manager.py              # archive + clear (default)
python query_manager.py --no-archive # just wipe, skip archiving
python query_manager.py --dir DIR    # target a different directory
```

Archives are capped at 20 (oldest pruned automatically).

### `raw_to_model.py`

Converts `raw_input/` into `model_input/`. Two modes, chosen automatically:

- **CSV+Text mode** (raw_desc.txt + raw_params.csv): LLM maps CSV columns to optimization parameters, then a second pass extracts additional numeric constants from the description text that aren't in the CSV (costs, rates, budgets, etc.). Merges both sources.
- **Text mode** (raw_desc.txt only): LLM extracts numeric parameters directly from the problem description. Best for simple problems where all data is stated in prose.

```
python raw_to_model.py               # process current_query/
python raw_to_model.py --dir DIR     # different problem directory
python raw_to_model.py --model MODEL # LLM model (default: gpt-4o)
```

---

## OptiMUS

A structured, multi-step pipeline that converts a natural-language optimization problem into a Gurobi solver script. It decomposes the problem into parameters, objectives, and constraints, formulates each mathematically, generates code, then executes and debugs it.

Steps 2 and 3 (objective + constraint extraction) run **in parallel** since they only depend on the problem description and parameters.

### Pipeline Steps

| Step | File | What it does |
|------|------|--------------|
| 1 | `step01_parameters.py` | Extract parameters from the problem description |
| 2 | `step02_objective.py` | Identify the optimization objective (**parallel with 3**) |
| 3 | `step03_constraints.py` | Extract constraints (**parallel with 2**) |
| 4 | `step04_constraint_model.py` | Formulate constraints in LaTeX |
| 5 | `step05_objective_model.py` | Formulate objective in LaTeX |
| 6 | `step06_target_code.py` | Generate Gurobi code for each constraint/objective |
| 7 | `step07_generate_code.py` | Assemble the complete solver script |
| 8 | `step08_execute_code.py` | Execute the script; if it errors, reflect and retry |

All step files live in `optimus_pipeline/`.

### Options

```
--dir DIR      Problem directory (default: current_query)
--model MODEL  LLM model (default: claude-sonnet-4-20250514)
```

### Programmatic Usage

```python
from optimus import run_pipeline

state = run_pipeline("current_query")
print(state["objective"])
```

### Output

Written to `current_query/optimus_output/`:

| File | Contents |
|------|----------|
| `code.py` | Generated Gurobi solver |
| `code_output.txt` | Solver output (optimal value) |
| `state_*.json` | Intermediate state at each pipeline step |
| `log.txt` | Full LLM interaction log |

---

## OptiMind

Microsoft Research's fine-tuned LLM for optimization. Given a natural-language problem description, it reasons step-by-step, produces a mathematical formulation, and generates executable GurobiPy code in a single pass.

- **Model:** `microsoft/OptiMind-SFT` (20B params, MoE architecture with 3.6B activated)
- **Base model:** `gpt-oss-20b-BF16`
- **Paper:** [OptiMind: Teaching LLMs to Think Like Optimization Experts](https://arxiv.org/abs/2509.22979)

### Deployment

The model is self-hosted on a GCP VM with a single NVIDIA L4 GPU (24GB VRAM), served via **llama.cpp** with Q4_K_M quantization.

**How we got it running:** The full-precision model is ~40GB (BF16), which exceeds the L4's 24GB VRAM. Standard serving frameworks (SGLang FP8, vLLM bitsandbytes, HuggingFace transformers + 4-bit) failed because they load full-precision weights into VRAM before quantizing. The solution was offline quantization via llama.cpp's GGUF format:

1. Build llama.cpp from source with CUDA support (`-DGGML_CUDA=ON`)
2. Download model from HuggingFace (~40GB safetensors, 10 shards)
3. Convert to GGUF Q8_0 intermediate using `convert_hf_to_gguf.py` (~22GB)
4. Quantize Q8_0 to Q4_K_M using `llama-quantize --allow-requantize` (~15GB)
5. Serve with `llama-server` with all layers offloaded to GPU (`--n-gpu-layers 99`)

The Q4_K_M model fits in 24GB VRAM. An **nginx** reverse proxy on port 80 forwards to llama-server on port 30000 (port 80 passes through campus/corporate firewalls that block non-standard ports). For step-by-step GCP setup, see **[docs/OPTIMIND_GOOGLE_CLOUD_SETUP.md](docs/OPTIMIND_GOOGLE_CLOUD_SETUP.md)**.

**Start the server** (on the VM):

```bash
cd ~/llama.cpp && nohup ./build/bin/llama-server \
  --model ~/optimind-sft-Q4_K_M.gguf \
  --host 0.0.0.0 --port 30000 \
  --n-gpu-layers 99 --ctx-size 35000 -fa on \
  > ~/server.log 2>&1 &
```

**Check if running:** `pgrep llama-server && echo 'running' || echo 'down'`

**Verify from your machine:** `curl http://<VM_EXTERNAL_IP>/v1/models`

**Stop the VM when done:** `gcloud compute instances stop <INSTANCE> --zone <ZONE> --project <PROJECT>`

### Configuration

Set in `.env`:

```
OPTIMIND_SERVER_URL=http://<VM_EXTERNAL_IP>/v1
```

### Options

```
--dir DIR          Problem directory (default: current_query)
--base-url URL     Server URL (default: from OPTIMIND_SERVER_URL env var)
```

### Programmatic Usage

```python
from optimind import run_pipeline

result = run_pipeline("current_query")
```

### Output

Written to `current_query/optimind_output/`:

| File | Contents |
|------|----------|
| `optimind_response.txt` | Full LLM response (reasoning + code) |
| `optimind_code.py` | Extracted GurobiPy solver code |
| `code_output.txt` | stdout/stderr from executing the code |
| `output_solution.txt` | Objective value (written by the executed code) |

---

## Judge

Compares solutions from OptiMUS and OptiMind and picks the best one. Does **not** generate the final report — that's the consultant's job.

### Decision Pipeline

1. **Data loading** -- Reads each solver's output and classifies execution status (`optimal`, `error`, `infeasible`, `unbounded`, etc.). Strips Gurobi noise (license banners, presolve stats) so the LLM sees only meaningful output.
2. **Programmatic fast-path** -- Clear-cut cases are resolved without an LLM call: if one solver crashed and the other succeeded, the working one wins immediately.
3. **LLM comparison** -- For ambiguous cases (both ran, or both failed), GPT-4o evaluates formulation correctness, implementation fidelity, and objective value.
4. **Sanity override** -- If the LLM contradicts programmatic evidence (e.g., picks a crashed solver), the decision is overridden.

### Options

```
--dir DIR      Problem directory (default: current_query)
--model MODEL  LLM model for judging (default: gpt-4o)
```

### Programmatic Usage

```python
from judge import compare_solutions

verdict = compare_solutions("current_query")
print(verdict["winner"], verdict["objective_value"])
```

### Output

Written to `current_query/final_output/verdict.json`.

**Solver status values:** `"success"` (optimal or feasible with numeric objective), `"error"` (code crashed), `"infeasible"`, `"unbounded"`, `"no_result"` (ran but no numeric objective), `"not_available"` (no output found).

---

## Consultant

Generates a professional, client-ready optimization report for the winning solution. Runs after the judge.

### What it produces

- **Executive Summary** — C-suite ready, no jargon. Specific numbers, actions, conditions, and bottom-line impact.
- **Baseline Comparison** — Compares the optimized solution against the client's current strategy (`model_input/baseline.txt`). Includes a metrics table with objective value, % improvement, and what specifically changes.
- **Key Recommendations** — Numbered, actionable implementation steps.
- **Technical Appendix** — Full mathematical formulation, decision variable values, active constraints, solver statistics (MIP gap, solve time, nodes), and the generated code.

### Baseline

The consultant reads `model_input/baseline.txt` for the client's current strategy. This file will eventually be auto-generated by a frontend LLM that separates the user's conversational input into a problem description and baseline. If `baseline.txt` is not present, the report is generated without the baseline comparison section.

### Options

```
--dir DIR      Problem directory (default: current_query)
--model MODEL  LLM model (default: gpt-4o)
```

### Programmatic Usage

```python
from consultant import generate_report

report = generate_report("current_query")
print(report["executive_summary"])
```

### Output

Written to `current_query/final_output/`:

| File | Contents |
|------|----------|
| `report.md` | Full professional Markdown report |
| `verdict.json` | Enriched with `executive_summary`, `has_baseline_comparison`, `gurobi_stats` |

---

## Known Issues & Future Work

### 1. OptiMind: Data Truncation in Generated Code

**Problem:** OptiMind receives the full raw parameter values in its prompt (e.g. 1,190-element vectors). The LLM cannot reproduce all values in its generated code, so it truncates arrays with comments like `# ... (remaining values omitted for brevity) ...`. The resulting code has mismatched array lengths and crashes with `IndexError` at runtime.

**Proposed fix — Layer 1 (Data injection):** Change `_read_problem()` in `optimind.py` so that instead of dumping raw data values into the prompt, it provides a summary (shapes, types, statistics, sample values) and instructs the LLM to generate code that reads `params.json` from disk at runtime. Copy `params.json` into the `optimind_output/` working directory so the generated code can find it. This prevents the LLM from ever needing to reproduce large data arrays inline.

**Proposed fix — Layer 2 (Debug-and-retry loop):** If the generated code fails execution, feed the code + traceback + params summary back to the LLM and ask it to fix the bug. Retry up to 2–3 times. This catches issues that data injection alone won't prevent: wrong variable names, off-by-one indexing, missing imports, Gurobi API misuse, etc. Small quantized models will sometimes produce imperfect code regardless of input quality, so a correction loop is essential.

**Implementation scope:** All changes are contained in `optimind.py`:
- Modify `_read_problem()` for data injection + summary
- Update `SYSTEM_PROMPT` to instruct the model to read from `params.json`
- Copy `params.json` into the `optimind_output/` working directory
- Add a `_build_fix_prompt()` helper and wire a retry loop around `_execute_code()` in `run_pipeline()`

### 2. OptiMUS: Fragile Constraint Formulation Parser

**Problem:** `extract_formulation_from_end()` in `step04_constraint_model.py` uses character-by-character string scanning to extract LaTeX formulations and JSON from LLM responses. This parser is brittle — when the LLM formats its response slightly differently than expected (e.g. omits `"NEW VARIABLES"`, uses different casing, or includes nested JSON in constraint descriptions), the string manipulation corrupts the text and the JSON extraction fails. A `.get()` fallback has been applied for `"NEW VARIABLES"`, but the underlying parser remains fragile.

**Proposed fix:** Replace the character-scanning parser with a more robust approach — e.g. regex-based extraction or a two-pass strategy that first extracts the JSON block, then pulls LaTeX strings from the JSON values directly, rather than stripping them from raw text before JSON parsing.

### 3. Constraint Description Format Inconsistency

**Problem:** The constraint extraction step (step 3) sometimes returns constraint descriptions as nested dicts (`{"Description": "...", "Formulation": null, "Code": null}`) instead of plain strings. When these get stringified and passed to the constraint formulation step (step 4), the LLM receives confusing input containing JSON artifacts, which degrades the quality of its response.

**Proposed fix:** Add a normalization step after constraint extraction that ensures every constraint's `description` field is a plain string. If it's a dict, extract the `"Description"` value.

### 4. Baseline Extraction from User Input

**Problem:** The consultant report supports baseline comparison (reading from `model_input/baseline.txt`), but this file must be manually created. There is no automated way to separate a user's conversational input into a problem description and a baseline strategy description.

**Proposed fix:** Add a preprocessing LLM step (or frontend logic) that takes the user's raw input and splits it into the problem description (`desc.txt`) and baseline strategy (`baseline.txt`) when a baseline is mentioned.

### 5. Inaccurate and Unreliable Parameter Extraction from CSVs

**Problem:** The `raw_to_model.py` parameter extraction frequently produces incorrect results, especially with structured or non-trivial CSV formats. Observed failure modes include:

- **Flattening structured data:** When a CSV uses a key-value layout (e.g. columns `Category, Parameter, Value, Unit`), the extractor treats the `Value` column as a single flat vector, mixing unrelated quantities (time strings, nurse counts, wage rates) into one parameter.
- **Missing parameters:** Numeric values present in the CSV (e.g. hourly wage rates, shift durations) are extracted as `0` or omitted entirely.
- **Type confusion:** Time strings like `"2:00"` end up in vectors declared as `"type": "integer"`, producing non-numeric data that downstream solvers cannot use.
- **Semantic misinterpretation:** The LLM does not reliably distinguish between index/label columns and data columns, or between rows that represent different logical entities (e.g. time period definitions vs. shift definitions vs. wage rates in the same CSV).

These issues stem from the single-prompt extraction approach: the LLM receives a column summary and must infer the semantic structure of the data in one pass, which it frequently gets wrong for anything beyond simple tabular layouts.

**Proposed fixes:**
- **Stronger prompt engineering:** Provide the LLM with more rows of sample data (not just 5) and explicit instructions for handling key-value / pivoted CSV formats.
- **Multi-pass extraction with validation:** After the first extraction pass, run a validation step that checks extracted values against the raw CSV (e.g. verify that numeric values actually appear in the source data, flag type mismatches).
- **Schema detection pre-step:** Before parameter extraction, have the LLM classify the CSV structure (flat table, key-value pairs, pivoted, hierarchical) and apply format-specific extraction logic.
- **Human-in-the-loop review:** For production use, present extracted parameters to the user for confirmation before passing them to solvers.
