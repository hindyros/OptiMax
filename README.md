# Optima

An ensemble NL-based optimization solver that compares solutions from OptiMUS and OptiMind, then uses an LLM judge to pick the best one.

Built for Treehacks 2026, with credit to original creators of OptiMUS and OptiMind.

## Setup

```bash
conda activate optima
pip install -r requirements.txt
```

Create a `.env` file in the project root (gitignored). Copy from `.env.example` and fill in your keys:

```
OPENAI_API_KEY=your-openai-key
OPENAI_ORG_ID=your-org-id
ANTHROPIC_API_KEY=your-anthropic-key
GROQ_API_KEY=your-groq-key
OPTIMIND_SERVER_URL=http://<VM_IP>/v1
```

### Gurobi license (for running generated solver code)

The pipeline generates and runs Gurobi (Python) code. You need a valid **Gurobi license** for that to work.

- **Get a license:** [Academic (free)](https://www.gurobi.com/academia/academic-program-and-licenses/) or [commercial](https://www.gurobi.com/licenses/). Use the [Gurobi User Portal](https://portal.gurobi.com/iam/licenses/list) to retrieve your license key or run `grbgetkey` (included with a full Gurobi install).
- **Set it up** so that `gurobipy` can find it:
  - **Option A (recommended):** Put your `gurobi.lic` file in a default location. On macOS: `/Library/gurobi/gurobi.lic` (system-wide) or `~/gurobi.lic` (your user only). On Linux: `/opt/gurobi/gurobi.lic` or `~/gurobi.lic`.
  - **Option B:** Point to the file with an environment variable (e.g. in `.env` or your shell profile):
    ```bash
    export GRB_LICENSE_FILE=/path/to/gurobi.lic
    ```
    The variable must point to the **file**, not the folder.

If the license is not set up, running the pipeline or executing generated code will fail with a Gurobi license error.

---

## Quick Start

```bash
# 1. Place your files in data_upload/
#    - A .txt file with the problem description (required)
#    - A .csv file with parameter data          (optional)

# 2. Run the pipeline
python main.py
```

That's it. The script clears the workspace, processes your inputs, runs both solvers, judges the results, and writes the final verdict to `current_query/final_output/`.

### Alternative: explicit file paths

```bash
python main.py --desc path/to/problem.txt
python main.py --desc path/to/problem.txt --data path/to/params.csv
```

### Pipeline steps (what `main.py` does)

| Step | What happens |
|------|-------------|
| 1 | Archive + clear `current_query/` |
| 2 | Copy uploaded files into `current_query/raw_input/` (renamed to `raw_desc.txt` / `raw_params.csv`) |
| 3 | `raw_to_model` — LLM converts raw inputs to `model_input/desc.txt` + `params.json` |
| 4 | `optimus` — structured multi-step OptiMUS solver |
| 5 | `optimind` — single-pass OptiMind LLM solver |
| 6 | `judge` — compares both solutions, picks a winner, generates explanation |

If one solver fails, the other's result is still judged. If both fail, you get a clear error.

### CLI options

```
python main.py                                    # use data_upload/
python main.py --desc problem.txt                 # explicit desc file
python main.py --desc problem.txt --data data.csv # desc + CSV
python main.py --no-archive                       # skip archiving old results
python main.py --dir other_dir                    # different workspace
```

### Directory structure

```
data_upload/             Drop your input files here
  problem.txt              Problem description (.txt, required)
  data.csv                 Parameter data (.csv, optional)

current_query/           Working directory (managed automatically)
  raw_input/               Copied from data_upload/
    raw_desc.txt
    raw_params.csv
  model_input/             Generated by raw_to_model
    desc.txt
    params.json
  optimus_output/          Generated by optimus
  optimind_output/         Generated by optimind
  final_output/            Generated by judge
    verdict.json             Structured result
    explanation.txt          NL explanation (executive + technical)
```

---

## Individual Scripts

Each pipeline step can also be run independently for debugging or development.

### `query_manager.py`

Manages the `current_query/` workspace. Archives the current contents to `query_history/<timestamp>/` then deletes all files while preserving the subdirectory structure.

```
python query_manager.py              # archive + clear (default)
python query_manager.py --no-archive # just wipe, skip archiving
python query_manager.py --dir DIR    # target a different directory
```

Archives are capped at 20 (oldest pruned automatically).

### `raw_to_model.py`

Converts `raw_input/` into `model_input/`. Two modes, chosen automatically:

- **CSV mode** (raw_desc.txt + raw_params.csv): LLM reasons over the description and data, mapping CSV columns to optimization parameters with types, shapes, and values.
- **Text mode** (raw_desc.txt only): LLM extracts numeric parameters directly from the problem description. Best for simple problems where all data is stated in prose.

```
python raw_to_model.py               # process current_query/
python raw_to_model.py --dir DIR     # different problem directory
python raw_to_model.py --model MODEL # LLM model (default: gpt-4o-mini)
```

---

## OptiMUS

A structured, multi-step pipeline that converts a natural-language optimization problem into a Gurobi solver script. It decomposes the problem into parameters, objectives, and constraints, formulates each mathematically, generates code, then executes and debugs it.

### Pipeline Steps

| Step | File | What it does |
|------|------|--------------|
| 1 | `step01_parameters.py` | Extract parameters from the problem description |
| 2 | `step02_objective.py` | Identify the optimization objective |
| 3 | `step03_constraints.py` | Extract constraints |
| 4 | `step04_constraint_model.py` | Formulate constraints in LaTeX |
| 5 | `step05_objective_model.py` | Formulate objective in LaTeX |
| 6 | `step06_target_code.py` | Generate Gurobi code for each constraint/objective |
| 7 | `step07_generate_code.py` | Assemble the complete solver script |
| 8 | `step08_execute_code.py` | Execute the script; if it errors, reflect and retry |

All step files live in `optimus_pipeline/`.

### Options

```
--dir DIR      Problem directory (default: current_query)
--model MODEL  LLM model (default: gpt-4o-mini)
```

### Programmatic Usage

```python
from optimus import run_pipeline

state = run_pipeline("current_query", model="gpt-4o-mini")
print(state["objective"])
```

### Output

Written to `current_query/optimus_output/`:

| File | Contents |
|------|----------|
| `code.py` | Generated Gurobi solver |
| `code_output.txt` | Solver output (optimal value) |
| `state_*.json` | Intermediate state at each pipeline step |
| `log.txt` | Full LLM interaction log |

---

## OptiMind

Microsoft Research's fine-tuned LLM for optimization. Given a natural-language problem description, it reasons step-by-step, produces a mathematical formulation, and generates executable GurobiPy code in a single pass.

- **Model:** `microsoft/OptiMind-SFT` (20B params, MoE architecture with 3.6B activated)
- **Base model:** `gpt-oss-20b-BF16`
- **Paper:** [OptiMind: Teaching LLMs to Think Like Optimization Experts](https://arxiv.org/abs/2509.22979)

### Deployment

The model is self-hosted on a GCP VM with a single NVIDIA L4 GPU (24GB VRAM), served via **llama.cpp** with Q4_K_M quantization.

**How we got it running:** The full-precision model is ~40GB (BF16), which exceeds the L4's 24GB VRAM. Standard serving frameworks (SGLang FP8, vLLM bitsandbytes, HuggingFace transformers + 4-bit) failed because they load full-precision weights into VRAM before quantizing. The solution was offline quantization via llama.cpp's GGUF format:

1. Build llama.cpp from source with CUDA support (`-DGGML_CUDA=ON`)
2. Download model from HuggingFace (~40GB safetensors, 10 shards)
3. Convert to GGUF Q8_0 intermediate using `convert_hf_to_gguf.py` (~22GB)
4. Quantize Q8_0 to Q4_K_M using `llama-quantize --allow-requantize` (~15GB)
5. Serve with `llama-server` with all layers offloaded to GPU (`--n-gpu-layers 99`)

The Q4_K_M model fits in 24GB VRAM. An **nginx** reverse proxy on port 80 forwards to llama-server on port 30000 (port 80 passes through campus/corporate firewalls that block non-standard ports). For step-by-step GCP setup, see **[docs/OPTIMIND_GOOGLE_CLOUD_SETUP.md](docs/OPTIMIND_GOOGLE_CLOUD_SETUP.md)**.

**Start the server** (on the VM):

```bash
cd ~/llama.cpp && nohup ./build/bin/llama-server \
  --model ~/optimind-sft-Q4_K_M.gguf \
  --host 0.0.0.0 --port 30000 \
  --n-gpu-layers 99 --ctx-size 4096 -fa on \
  > ~/server.log 2>&1 &
```

**Check if running:** `pgrep llama-server && echo 'running' || echo 'down'`

**Verify from your machine:** `curl http://<VM_EXTERNAL_IP>/v1/models`

**Stop the VM when done:** `gcloud compute instances stop <INSTANCE> --zone <ZONE> --project <PROJECT>`

### Configuration

Set in `.env`:

```
OPTIMIND_SERVER_URL=http://<VM_EXTERNAL_IP>/v1
```

### Options

```
--dir DIR          Problem directory (default: current_query)
--base-url URL     Server URL (default: from OPTIMIND_SERVER_URL env var)
```

### Programmatic Usage

```python
from optimind import run_pipeline

result = run_pipeline("current_query")
```

### Output

Written to `current_query/optimind_output/`:

| File | Contents |
|------|----------|
| `optimind_response.txt` | Full LLM response (reasoning + code) |
| `optimind_code.py` | Extracted GurobiPy solver code |
| `code_output.txt` | stdout/stderr from executing the code |
| `output_solution.txt` | Objective value (written by the executed code) |

---

## Judge

Compares solutions from OptiMUS and OptiMind, picks the best one, and generates a professional natural-language explanation.

### Decision Pipeline

1. **Data loading** -- Reads each solver's output and classifies execution status (`optimal`, `error`, `infeasible`, `unbounded`, etc.). Strips Gurobi noise (license banners, presolve stats) so the LLM sees only meaningful output.
2. **Programmatic fast-path** -- Clear-cut cases are resolved without an LLM call: if one solver crashed and the other succeeded, the working one wins immediately.
3. **LLM comparison** -- For ambiguous cases (both ran, or both failed), GPT-4o evaluates formulation correctness, implementation fidelity, and objective value.
4. **Sanity override** -- If the LLM contradicts programmatic evidence (e.g., picks a crashed solver), the decision is overridden.
5. **Explanation generation** -- A second LLM call produces an executive summary and technical appendix for the winning solution.

### Options

```
--dir DIR      Problem directory (default: current_query)
--model MODEL  LLM model for judging (default: gpt-4o)
```

### Programmatic Usage

```python
from judge import compare_solutions

verdict = compare_solutions("current_query")
print(verdict["winner"], verdict["objective_value"])
```

### Output

Written to `current_query/final_output/`:

| File | Contents |
|------|----------|
| `verdict.json` | Structured result consumed by the frontend |
| `explanation.txt` | Full NL explanation (executive summary + technical appendix) |

### `verdict.json` Schema

```json
{
    "winner": "optimus",
    "objective_value": 280.0,
    "direction": "maximize",
    "solvers": {
        "optimus":  { "status": "success", "objective_value": 280.0 },
        "optimind": { "status": "not_available", "objective_value": null }
    },
    "reasoning": "Why this solver was chosen...",
    "optimus_assessment": "Assessment of OptiMUS solution...",
    "optimind_assessment": "Assessment of OptiMind solution...",
    "explanation": "Executive summary text...",
    "technical_details": "Mathematical formulation, code, solver output..."
}
```

**Solver status values:** `"success"` (optimal or feasible with numeric objective), `"error"` (code crashed), `"infeasible"`, `"unbounded"`, `"no_result"` (ran but no numeric objective), `"not_available"` (no output found).
