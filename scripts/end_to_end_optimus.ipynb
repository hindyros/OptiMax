{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1eee6751",
      "metadata": {},
      "source": [
        "# End-to-end OptiMUS pipeline\n",
        "\n",
        "This notebook runs the full flow: **problem description + data** → **OptiMUS inputs** → **OptiMUS run** → **solution**.\n",
        "\n",
        "1. **Setup** – paths and imports  \n",
        "2. **Inputs** – paths to data and description  \n",
        "3. **Step 1** – Generate OptiMUS inputs (desc.txt, params.json, labels.json) from description + data  \n",
        "4. **Step 2** – Run OptiMUS pipeline (extract objective, constraints, generate code, solve)  \n",
        "5. **Step 3** – Load and display the solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "05cd59af",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /Users/hindy/Desktop/OptiMUS\n"
          ]
        }
      ],
      "source": [
        "# Setup: project root and imports\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "# Project root (parent of scripts/)\n",
        "NOTEBOOK_DIR = Path.cwd()\n",
        "PROJECT_ROOT = NOTEBOOK_DIR.parent if (NOTEBOOK_DIR / \"data_to_optimus.py\").exists() else NOTEBOOK_DIR\n",
        "os.chdir(PROJECT_ROOT)\n",
        "sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54bbe3c4",
      "metadata": {},
      "source": [
        "## Inputs\n",
        "\n",
        "Set paths to your **data file(s)** (CSV/Excel) and **problem description** (text or path to `.txt`). Use a single path or a list of paths for multiple datasets (e.g. `DATA_PATH = [\"inventory.csv\", \"stores.csv\"]`). Output directory is where OptiMUS input files will be written."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "e6787ad3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths (relative to project root)\n",
        "# Data and description can be anywhere; output goes to current_query so OptiMUS runs with no --dir\n",
        "DATA_PATH = \"/Users/hindy/Desktop/OptiMUS/operations_data/inventory_monitoring.csv\"\n",
        "DESCRIPTION_PATH = \"/Users/hindy/Desktop/OptiMUS/operations_data/desc.txt\"\n",
        "OUTPUT_DIR = \"current_query\"\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "# Optional: use --no-llm for no API calls, or --simple for one param per column\n",
        "USE_EXPERT = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcc66c0c",
      "metadata": {},
      "source": [
        "## Step 1: Generate OptiMUS inputs\n",
        "\n",
        "Convert **description + data** into `desc.txt`, `params.json`, and `labels.json` in the output directory. Uses expert (LLM) extraction by default to map columns to parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "ba39177d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 31 rows, 9 columns from /Users/hindy/Desktop/OptiMUS/operations_data/inventory_monitoring.csv\n",
            "Extracting parameters with expert (LLM) reasoning over description and dataset...\n",
            "Wrote OptiMUS inputs to current_query/\n",
            "  desc.txt, params.json, labels.json (9 parameters)\n",
            "\n",
            "Next: run OptiMUS with:\n",
            "  python optimus.py\n",
            "OptiMUS inputs written to current_query\n"
          ]
        }
      ],
      "source": [
        "import importlib.util\n",
        "_spec = importlib.util.spec_from_file_location(\n",
        "    \"data_to_optimus\",\n",
        "    Path(PROJECT_ROOT) / \"scripts\" / \"data_to_optimus.py\",\n",
        ")\n",
        "_data_to_optimus = importlib.util.module_from_spec(_spec)\n",
        "_spec.loader.exec_module(_data_to_optimus)\n",
        "\n",
        "_data_to_optimus.run(\n",
        "    DATA_PATH,\n",
        "    DESCRIPTION_PATH,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    model=MODEL,\n",
        "    use_expert=USE_EXPERT,\n",
        "    no_llm=False,\n",
        ")\n",
        "print(\"OptiMUS inputs written to\", OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91f06210",
      "metadata": {},
      "source": [
        "## Step 2: Run OptiMUS pipeline\n",
        "\n",
        "Run the full OptiMUS pipeline on the generated folder: extract objective and constraints, formulate them, generate Gurobi code, execute and debug until a solution is found."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "db65e92b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'description': '\"The goal is to reduce stockouts on high-demand products, minimize holding and markdown costs, and avoid costly emergency shipments.\"', 'formulation': None, 'code': None}\n",
            "_________________________ get_constraints _________________________\n",
            "[{'description': 'Stock levels for each product at each store must be non-negative', 'formulation': None, 'code': None}, {'description': 'Total inventory at each warehouse must not exceed the warehouse capacity', 'formulation': None, 'code': None}, {'description': 'Reorder points for each product must be set above the minimum stock level to avoid stockouts', 'formulation': None, 'code': None}, {'description': 'Product allocations from warehouses to stores must not exceed the current stock levels', 'formulation': None, 'code': None}, {'description': 'Supplier lead times must be accounted for in the replenishment schedule', 'formulation': None, 'code': None}, {'description': 'Expiry dates must be considered to manage perishable items effectively', 'formulation': None, 'code': None}, {'description': 'Total replenishment orders must align with available budget or cost constraints', 'formulation': None, 'code': None}, {'description': 'Markdown prices must not reduce the selling price below a certain threshold to maintain profit margins', 'formulation': None, 'code': None}]\n",
            "x {'shape': '[NumberOfProducts, NumberOfWarehouses]', 'type': 'continuous', 'definition': 'The inventory level of each product in each warehouse'}\n",
            "y {'shape': '[NumberOfProducts, NumberOfStores]', 'type': 'continuous', 'definition': 'The quantity of each product allocated from the warehouse to each store'}\n",
            "TotalReplenishmentCost {'shape': [], 'type': 'continuous', 'definition': 'The total cost incurred by replenishing inventory across all products and warehouses'}\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
            "File \u001b[0;32m~/Coding/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1050\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1050\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n",
            "File \u001b[0;32m~/Coding/anaconda3/lib/python3.11/site-packages/httpx/_models.py:763\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[0;32m--> 763\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.openai.com/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptimus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_pipeline\n\u001b[0;32m----> 3\u001b[0m state \u001b[38;5;241m=\u001b[39m run_pipeline(\n\u001b[1;32m      4\u001b[0m     problem_dir\u001b[38;5;241m=\u001b[39mOUTPUT_DIR,\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m=\u001b[39mMODEL,\n\u001b[1;32m      6\u001b[0m     error_correction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline finished. State keys:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(state\u001b[38;5;241m.\u001b[39mkeys()))\n",
            "File \u001b[0;32m~/Desktop/OptiMUS/optimus.py:100\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[0;34m(problem_dir, rag_mode, model, error_correction)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Step 4: Formulate constraints (LaTeX)\u001b[39;00m\n\u001b[1;32m     99\u001b[0m state \u001b[38;5;241m=\u001b[39m load_state(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(run_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_3_constraints.json\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 100\u001b[0m constraints, variables \u001b[38;5;241m=\u001b[39m get_constraint_formulations(\n\u001b[1;32m    101\u001b[0m     state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    102\u001b[0m     state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    103\u001b[0m     state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstraints\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    104\u001b[0m     check\u001b[38;5;241m=\u001b[39merror_correction,\n\u001b[1;32m    105\u001b[0m     logger\u001b[38;5;241m=\u001b[39mlogger,\n\u001b[1;32m    106\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    107\u001b[0m     rag_mode\u001b[38;5;241m=\u001b[39mrag_mode,\n\u001b[1;32m    108\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m    109\u001b[0m )\n\u001b[1;32m    110\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstraints\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m constraints\n\u001b[1;32m    111\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariables\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m variables\n",
            "File \u001b[0;32m~/Desktop/OptiMUS/optimus_pipeline/step04_constraint_model.py:448\u001b[0m, in \u001b[0;36mget_constraint_formulations\u001b[0;34m(desc, params, constraints, model, check, logger, rag_mode, labels)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m k \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    440\u001b[0m     p \u001b[38;5;241m=\u001b[39m prompt_constraints_q\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    441\u001b[0m         description\u001b[38;5;241m=\u001b[39mdesc,\n\u001b[1;32m    442\u001b[0m         params\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(params, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m         question\u001b[38;5;241m=\u001b[39mq[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    446\u001b[0m     )\n\u001b[0;32m--> 448\u001b[0m     x \u001b[38;5;241m=\u001b[39m get_response(p, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m    450\u001b[0m     valid, res \u001b[38;5;241m=\u001b[39m q[\u001b[38;5;241m1\u001b[39m](x, params, \u001b[38;5;28mvars\u001b[39m, constraints, c)\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28mprint\u001b[39m(valid)\n",
            "File \u001b[0;32m~/Desktop/OptiMUS/optimus_utils.py:219\u001b[0m, in \u001b[0;36mget_response\u001b[0;34m(prompt, model)\u001b[0m\n\u001b[1;32m    213\u001b[0m     chat_completion \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    214\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}],\n\u001b[1;32m    215\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    216\u001b[0m     )\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chat_completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m--> 219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _retry_llm_call(_call)\n",
            "File \u001b[0;32m~/Desktop/OptiMUS/optimus_utils.py:166\u001b[0m, in \u001b[0;36m_retry_llm_call\u001b[0;34m(callable_fn, max_attempts, base_delay)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_attempts):\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m callable_fn()\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    168\u001b[0m         last_error \u001b[38;5;241m=\u001b[39m e\n",
            "File \u001b[0;32m~/Desktop/OptiMUS/optimus_utils.py:213\u001b[0m, in \u001b[0;36mget_response.<locals>._call\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m():\n\u001b[0;32m--> 213\u001b[0m     chat_completion \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    214\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}],\n\u001b[1;32m    215\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    216\u001b[0m     )\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chat_completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
            "File \u001b[0;32m~/Coding/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py:286\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/Coding/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:1192\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m not_given,\n\u001b[1;32m   1190\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m   1191\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1194\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m   1195\u001b[0m             {\n\u001b[1;32m   1196\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   1197\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1198\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m   1199\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1200\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   1201\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   1202\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1203\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   1204\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m   1205\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1206\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   1207\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m   1208\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   1209\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   1210\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[1;32m   1211\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1212\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_cache_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_cache_key,\n\u001b[1;32m   1213\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_cache_retention\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_cache_retention,\n\u001b[1;32m   1214\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[1;32m   1215\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   1216\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msafety_identifier\u001b[39m\u001b[38;5;124m\"\u001b[39m: safety_identifier,\n\u001b[1;32m   1217\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   1218\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   1219\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   1220\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m   1221\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1222\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   1223\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1224\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1225\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   1226\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   1227\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1228\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   1229\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbosity\u001b[39m\u001b[38;5;124m\"\u001b[39m: verbosity,\n\u001b[1;32m   1230\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb_search_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: web_search_options,\n\u001b[1;32m   1231\u001b[0m             },\n\u001b[1;32m   1232\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsStreaming\n\u001b[1;32m   1233\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[1;32m   1234\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsNonStreaming,\n\u001b[1;32m   1235\u001b[0m         ),\n\u001b[1;32m   1236\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1237\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1238\u001b[0m         ),\n\u001b[1;32m   1239\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   1240\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1241\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m   1242\u001b[0m     )\n",
            "File \u001b[0;32m~/Coding/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1297\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, content, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1288\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing raw bytes as `body` is deprecated and will be removed in a future version. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease pass raw bytes via the `content` parameter instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1291\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m   1292\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   1293\u001b[0m     )\n\u001b[1;32m   1294\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1295\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, content\u001b[38;5;241m=\u001b[39mcontent, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1296\u001b[0m )\n\u001b[0;32m-> 1297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
            "File \u001b[0;32m~/Coding/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1056\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1055\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sleep_for_retry(\n\u001b[1;32m   1057\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1058\u001b[0m         max_retries\u001b[38;5;241m=\u001b[39mmax_retries,\n\u001b[1;32m   1059\u001b[0m         options\u001b[38;5;241m=\u001b[39minput_options,\n\u001b[1;32m   1060\u001b[0m         response\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[1;32m   1061\u001b[0m     )\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
            "File \u001b[0;32m~/Coding/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1096\u001b[0m, in \u001b[0;36mSyncAPIClient._sleep_for_retry\u001b[0;34m(self, retries_taken, max_retries, options, response)\u001b[0m\n\u001b[1;32m   1093\u001b[0m timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_retry_timeout(remaining_retries, options, response\u001b[38;5;241m.\u001b[39mheaders \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1094\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[0;32m-> 1096\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from optimus import run_pipeline\n",
        "\n",
        "state = run_pipeline(\n",
        "    problem_dir=OUTPUT_DIR,\n",
        "    model=MODEL,\n",
        "    error_correction=True,\n",
        ")\n",
        "print(\"Pipeline finished. State keys:\", list(state.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ef941de",
      "metadata": {},
      "source": [
        "## Step 3: Load and display the solution\n",
        "\n",
        "Read the solver output and optional solution file from `output/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "aeddc189",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "code_output.txt not found\n"
          ]
        }
      ],
      "source": [
        "output_dir = Path(PROJECT_ROOT) / OUTPUT_DIR / \"output\"\n",
        "\n",
        "# Solver stdout (often contains \"Optimal Objective Value: ...\")\n",
        "code_output = output_dir / \"code_output.txt\"\n",
        "if code_output.exists():\n",
        "    print(\"--- code_output.txt ---\")\n",
        "    print(code_output.read_text())\n",
        "else:\n",
        "    print(\"code_output.txt not found\")\n",
        "\n",
        "# Written optimal value (if status was OPTIMAL)\n",
        "solution_file = output_dir / \"output_solution.txt\"\n",
        "if solution_file.exists():\n",
        "    print(\"\\n--- output_solution.txt ---\")\n",
        "    print(solution_file.read_text())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
